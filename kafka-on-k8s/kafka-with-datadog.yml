apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-cfg
  namespace: support
data: 
  custom-server.properties: |    
    broker.id.generation.enable=true
    group.initial.rebalance.delay.ms=0
    leader.imbalance.check.interval.seconds=300
    leader.imbalance.per.broker.percentage=10
    listeners=PLAINTEXT://:9092
    log.dirs=/tmp/kafka-logs
    log.retention.check.interval.ms=300000
    log.retention.hours=168
    log.segment.bytes=1073741824    
    num.io.threads=8
    num.network.threads=3    
    num.partitions=1
    num.recovery.threads.per.data.dir=1
    offsets.topic.replication.factor=3
    replica.socket.receive.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600
    socket.send.buffer.bytes=102400
    transaction.state.log.replication.factor=1
    transaction.state.log.min.isr=1
    unclean.leader.election.enable=true
    zookeeper.connect=zookeeper-0.support:2181,zookeeper-1.support:2181,zookeeper-2.support:2181
    zookeeper.connection.timeout.ms=6000    
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: support
spec:
  selector:
    matchLabels:
      app: kafka
  serviceName: kafka
  replicas: 4
  template:
    metadata:
      labels:
        app: kafka
      annotations:
        linkerd.io/inject: enabled
        config.linkerd.io/proxy-log-level: debug
        ad.datadoghq.com/k8skafka.check_names: '["kafka", "kafka_consumer", "kafka_consumer"]'
        ad.datadoghq.com/k8skafka.instances: |
          [{
            "host": "%%host%%",
            "port": "10004",
            "tags": ["kafka:broker", "namespace:staging"]
          }, {
            "kafka_connect_str": "%%host%%:9092",
            "consumer_groups": {
              "engineTracesConsumer": { "engine-traces": [] },
              "engine-notifications-pager-duty": { "engine-notifications-pager-duty": [] },
              "engine-notifications-slack": { "engine-notifications-slack": [] }
            },
            "tags": ["namespace:staging"]
          }, {
            "kafka_connect_str": "%%host%%:9092",
            "consumer_groups": {
              "engine-datadogforwarder-20191125": {
                "engine-datadogforwarder-20191125-datadogStatsByServiceId-repartition": [],
                "engine-reports-stats": []
              }
            },
            "tags": ["namespace:staging"]
          }]
        # Note that each kafka_consumer check instance should collect at most
        # 500 partition contexts of data by default. A partition context is NOT
        # a partition, despite what some misleading Datadog documentation may
        # have you believe.
        #
        # To compute the partition context count for a kafka_consumer check
        # instance, count how many unique (consumer_group, topic, partition)
        # triplets are specified in your check, and add that to how many
        # unique (topic, partition) pairs are specified in your check. E.g. if
        # your check has
        # "consumer_groups": {
        #   "group_1": {
        #     "topic_1": [],
        #     "topic_2": []
        #   },
        #   "group_2": {
        #     "topic_1": []
        #   }
        # }
        # and each topic has 4 partitions, then the partition context count is
        # (1*2*4 + 1*1*4) + (2*4) = 20.
        #
        # If we need more, adjust max_partitition_contexts in the check's
        # init_config, or add another kafka_consumer check instance. (The
        # second is preferred to avoid individual check runs from taking too
        # long.)
        #
        # Also relevant: if you go over the limit, the kafka_consumer check
        # doesn't drop the metrics, but prints out a warning on every check
        # run.
        ad.datadoghq.com/k8skafka.init_configs: |
          [{
            "is_jmx": true,
            "collect_default_metrics": true
          }, {}, {}]        
    spec:
      terminationGracePeriodSeconds: 30
      containers:
      - name: kafka
        image: cpretzer/kafka:2.2.0
        imagePullPolicy: Always
        ports:
        - containerPort: 9092
          name: server
        - containerPort: 10004
          name: jmx
        volumeMounts:
        - name: logs
          mountPath: /tmp
        - name: conf
          mountPath: /tmp/kafka-cfg
        env:
          - name: KAFKA_POD_NAME
            valueFrom: 
              fieldRef:
                fieldPath: metadata.name
          - name: KAFKA_NAMESPACE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KAFKA_OPTS
            value: "-Dlogging.level=DEBUG -Dsun.net.inetaddr.ttl=10"
          - name: JAVA_DEBUG_PORT
            value: "10003"
          - name: JMX_PORT
            value: "10004"
          - name: KAFKA_DEBUG
            value: "true"
        args: 
          - "--override"
          - "advertised.listeners=PLAINTEXT://$(KAFKA_POD_NAME).$(KAFKA_NAMESPACE_NAME).svc.cluster.local:9092"
          # - "--override"
          # - "broker.id=${KAFKA_POD_NAME##*-}"
      volumes:
      - name: conf
        configMap:
          name: kafka-cfg
  volumeClaimTemplates:
  - metadata:
      name: logs
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 8Gi
---
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: support
  labels:
    name: kafka
    app: kafka
spec:
  ports:
  - name: server
    port: 9092
  - port: 10004
    name: jmx    
  clusterIP: None
  selector:
    statefulset.kubernetes.io/pod-name: kafka
---    
apiVersion: v1
kind: Service
metadata:
  name: kafka-0
  namespace: support
  labels:
    name: kafka-0
    app: kafka
spec:
  ports:
  - name: server
    port: 9092
  - port: 10004
    name: jmx
  selector:
    statefulset.kubernetes.io/pod-name: kafka-0
---    
apiVersion: v1
kind: Service
metadata:
  name: kafka-1
  namespace: support
  labels:
    name: kafka-1
    app: kafka
spec:
  ports:
  - name: server
    port: 9092
  - port: 10004
    name: jmx
  selector:
    statefulset.kubernetes.io/pod-name: kafka-1
---    
apiVersion: v1
kind: Service
metadata:
  name: kafka-2
  namespace: support
  labels:
    name: kafka-2
    app: kafka
spec:
  ports:
  - name: server
    port: 9092
  - port: 10004
    name: jmx
  selector:
    statefulset.kubernetes.io/pod-name: kafka-2
---    
apiVersion: v1
kind: Service
metadata:
  name: kafka-3
  namespace: support
  labels:
    name: kafka-3
    app: kafka
spec:
  ports:
  - name: server
    port: 9092
  - port: 10004
    name: jmx
  selector:
    statefulset.kubernetes.io/pod-name: kafka-3
---    